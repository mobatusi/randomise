{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "%autosave 0\n",
    "#!/usr/bin/env python\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import json\n",
    "import numpy as np\n",
    "from create_flame_model_files import create_flame_model_files\n",
    "__version__ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "1. 'bids_dir', 'The directory with the input dataset           'formatted according to the BIDS standard.'\n",
    "2. 'output_dir', 'The directory where the output files '        'should be stored. If you are running group level analysis '   'this folder should be prepopulated with the results of the'  'participant level analysis.'\n",
    "3. 'working_dir', 'The directory where intermediary files '          'are stored while working on them.'\n",
    "4. 'analysis_level', 'Level of the analysis that will be performed.  'Multiple participant level analyses can be run independently '     '(in parallel using the same output_dir. Use test_model to         'the model and contrast files, but not run the anlaysis.',         choices=['participant', 'group', 'test_model']\n",
    "5. 'model_file', 'JSON file describing the model and contrasts'  'that should be.'\n",
    "'--num_iterations', 'Number of iterations used by randomise.',      default=10000, type=int\n",
    "'--num_processors', 'Number of processors used at a time for randomise',default=1, type=int'-v', '--version', action='version', version='BIDS-App example version {}'.format(__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def model_setup(in_model_file, in_bids_dir, model_files_outdir):\n",
    "\n",
    "    # load in the model\n",
    "    with open(in_model_file) as model_fd:\n",
    "        model_dict = json.load(model_fd)\n",
    "\n",
    "    # parse the model string to determine which columns of the pheno\n",
    "    # file we are interested in\n",
    "    in_columns = model_dict[\"model\"].replace(\"-1\", \"\").replace(\"-\", \"+\").split(\"+\")\n",
    "    t_columns = []\n",
    "    for column in in_columns:\n",
    "        if '*' in column:\n",
    "            t_columns += column.split(\"*\")\n",
    "        else:\n",
    "            t_columns.append(column)\n",
    "        in_columns = list(set(t_columns))\n",
    "\n",
    "    # read in the phenotypic file\n",
    "    pheno_df = pd.read_csv(os.path.join(in_bids_dir, 'participants.tsv'), sep='\\t')\n",
    "\n",
    "    # reduce the file to just the columns that we are interested in\n",
    "    pheno_df = pheno_df[['participant_id'] + in_columns]\n",
    "\n",
    "    # remove rows that have empty elements\n",
    "    pheno_df = pheno_df.dropna()\n",
    "\n",
    "    # go through data, verify that we can find a corresponding entry in\n",
    "    # the pheno file, and keep track of the indices so that we can\n",
    "    # reorder the pheno to correspond\n",
    "    t_file_list = []\n",
    "    pheno_key_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(in_bids_dir):\n",
    "        for filename in files:\n",
    "\n",
    "            if not filename.endswith(\".nii.gz\"):\n",
    "                continue\n",
    "\n",
    "            # make a dictionary from the key-value chunks\n",
    "            f_chunks = (filename.split(\".\")[0]).split(\"_\")\n",
    "            f_dict = {chunk.split(\"-\")[0]: \"-\".join(chunk.split(\"-\")[1:]) for chunk in f_chunks[:-1]}\n",
    "\n",
    "            if not f_dict['ses']:\n",
    "                f_dict['ses'] = '1'\n",
    "\n",
    "            f_participant_name = \"-\".join([\"sub\", f_dict[\"sub\"]])\n",
    "\n",
    "            # find the row of the pheno_df that corresponds to the file and save it to pheno_key_list\n",
    "            participant_index = [index for index, participant_id in enumerate(pheno_df[\"participant_id\"])\n",
    "                                 if participant_id == f_participant_name]\n",
    "\n",
    "            if len(participant_index) == 0:\n",
    "                print(\"Could not find entry in phenotype file for {0}, dropping it.\".format(\n",
    "                    os.path.join(root, filename)))\n",
    "            elif len(participant_index) > 1:\n",
    "                raise ValueError(\"Found multiple entries for {0} in {1}\".format(f_participant_name,\n",
    "                             os.path.join(in_bids_dir, 'participants.tsv')))\n",
    "            else:\n",
    "                pheno_key_list.append(participant_index[0])\n",
    "                t_file_list.append(os.path.join(root, filename))\n",
    "\n",
    "    # now create the design.mat file\n",
    "\n",
    "    # remove participant_id column\n",
    "    pheno_df = pheno_df[in_columns]\n",
    "\n",
    "    # reduce to the rows that we are using, and reorder to match the file list\n",
    "    pheno_df = pheno_df.iloc[pheno_key_list, :]\n",
    "\n",
    "    print \"{0} rows in design matrix\".format(len(pheno_df.index))\n",
    "\n",
    "    # de-mean all numeric columns, we expect categorical variables to be encoded with strings\n",
    "    for df_ndx in pheno_df.columns:\n",
    "        if np.issubdtype(pheno_df[df_ndx].dtype, np.number):\n",
    "            pheno_df[df_ndx] -= pheno_df[df_ndx].mean()\n",
    "\n",
    "    # use patsy to create the design matrix\n",
    "    design = patsy.dmatrix(model_dict[\"model\"], pheno_df, NA_action='raise')\n",
    "    column_names = design.design_info.column_names\n",
    "\n",
    "    print('Model terms: {0}'.format(', '.join(column_names)))\n",
    "\n",
    "    # create contrasts\n",
    "    if model_dict[\"contrasts\"]:\n",
    "        contrast_dict = {}\n",
    "        t_num_contrasts = 0\n",
    "\n",
    "        for k in model_dict[\"contrasts\"]:\n",
    "            t_num_contrasts += 1\n",
    "            try:\n",
    "                contrast_dict[k] = [n if n != -0 else 0\n",
    "                                    for n in design.design_info.linear_constraint(k.encode('ascii')).coefs[0]]\n",
    "            except patsy.PatsyError as e:\n",
    "                if 'token' in e.message:\n",
    "                    print(\"A token in contrast \\'{0}\\' could not be found, should only include tokens from {1}\".format(\n",
    "                        k, ', '.join(column_names)))\n",
    "                raise\n",
    "    else:\n",
    "        raise ValueError('Model file {0} is missing contrasts'.format(model_file))\n",
    "\n",
    "    num_subjects = len(t_file_list)\n",
    "    t_mat_file, t_grp_file, t_con_file, t_fts_file = create_flame_model_files(design, column_names,\n",
    "                                                                              contrast_dict, None, [],\n",
    "                                                                              None, [1] * num_subjects,\n",
    "                                                                              \"Treatment\",\n",
    "                                                                              \"randomise_pipe_model\",\n",
    "                                                                              [], model_files_outdir)\n",
    "\n",
    "    return t_file_list, t_num_contrasts, t_mat_file, t_con_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_file = './test_data/nfb_dcb/model_both.json'\n",
    "bids_dir =  './test_data/nfb_dcb/'\n",
    "working_dir = './test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00033747/ses-NFB3/sub-A00033747_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00035072/ses-NFB3/sub-A00035072_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00035827/ses-NFB3/sub-A00035827_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00035840/ses-NFB3/sub-A00035840_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00037112/ses-NFB3/sub-A00037112_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00037511/ses-NFB3/sub-A00037511_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00039391/ses-NFB3/sub-A00039391_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00039431/ses-NFB3/sub-A00039431_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00040524/ses-NFB3/sub-A00040524_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00040573/ses-NFB3/sub-A00040573_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00040623/ses-NFB3/sub-A00040623_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00040640/ses-NFB3/sub-A00040640_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00040944/ses-NFB3/sub-A00040944_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00043677/ses-NFB3/sub-A00043677_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00043721/ses-NFB3/sub-A00043721_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00043722/ses-NFB3/sub-A00043722_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00051548/ses-NFB3/sub-A00051548_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00051676/ses-NFB3/sub-A00051676_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00051927/ses-NFB3/sub-A00051927_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00052125/ses-NFB3/sub-A00052125_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00054441/ses-NFB3/sub-A00054441_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00055447/ses-NFB3/sub-A00055447_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00056627/ses-NFB3/sub-A00056627_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "Could not find entry in phenotype file for ./test_data/nfb_dcb/sub-A00057444/ses-NFB3/sub-A00057444_ses-NFB3_pipe-cpac_dcb.nii.gz, dropping it.\n",
      "74 rows in design matrix\n",
      "Model terms: Sex[FEMALE], Sex[MALE], Handedness[T.LEFT], Handedness[T.RIGHT], Group, Group:Sex[T.MALE], Age, Mean_Relative_RMS_Displacement\n"
     ]
    }
   ],
   "source": [
    "file_list, num_contrasts, mat_file, con_file = model_setup(model_file, bids_dir, working_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='ABIDE Group Analysis Runner')\n",
    "\n",
    "    parser.add_argument('bids_dir', help='The directory with the input dataset '\n",
    "                        'formatted according to the BIDS standard.')\n",
    "    parser.add_argument('output_dir', help='The directory where the output files '\n",
    "                        'should be stored. If you are running group level analysis '\n",
    "                        'this folder should be prepopulated with the results of the'\n",
    "                        'participant level analysis.')\n",
    "    parser.add_argument('working_dir', help='The directory where intermediary files '\n",
    "                        'are stored while working on them.')\n",
    "    parser.add_argument('analysis_level', help='Level of the analysis that will be performed. '\n",
    "                        'Multiple participant level analyses can be run independently '\n",
    "                        '(in parallel) using the same output_dir. Use test_model to generate'\n",
    "                        'the model and contrast files, but not run the analysis.',\n",
    "                        choices=['participant', 'group', 'test_model'])\n",
    "    parser.add_argument('model_file', help='JSON file describing the model and contrasts'\n",
    "                        'that should be.')\n",
    "    parser.add_argument('--num_iterations', help='Number of iterations used by randomise.',\n",
    "                        default=10000, type=int)\n",
    "    parser.add_argument('--num_processors', help='Number of processors used at a time for randomise',\n",
    "                        default=1, type=int)\n",
    "    parser.add_argument('-v', '--version', action='version',\n",
    "                        version='BIDS-App example version {}'.format(__version__))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_file = args.model_file\n",
    "    if not os.path.isfile(model_file):\n",
    "        print(\"Could not find model file {0}\".format(model_file))\n",
    "        sys.exit(1)\n",
    "\n",
    "    output_dir = args.output_dir.rstrip('/')\n",
    "    if not os.path.isdir(output_dir):\n",
    "        print(\"Could not find output directory {0}\".format(output_dir))\n",
    "        sys.exit(1)\n",
    "\n",
    "    working_dir = args.working_dir.rstrip('/')\n",
    "    if not os.path.isdir(working_dir):\n",
    "        print(\"Could not find working directory {0}\".format(working_dir))\n",
    "        sys.exit(1)\n",
    "\n",
    "    bids_dir = args.bids_dir.rstrip('/')\n",
    "    if not os.path.isdir(working_dir):\n",
    "        print(\"Could not find bids directory {0}\".format(bids_dir))\n",
    "        sys.exit(1)\n",
    "\n",
    "    num_iterations = 10000\n",
    "    if args.num_iterations:\n",
    "        num_iterations = int(args.num_iterations)\n",
    "\n",
    "    num_processors = 1\n",
    "    if args.num_processors:\n",
    "        num_processors = int(args.num_processors)\n",
    "\n",
    "    print (\"\\n\")\n",
    "    print (\"## Running randomize pipeline with parameters:\")\n",
    "    print (\"Output directory: {0}\".format(bids_dir))\n",
    "    print (\"Output directory: {0}\".format(output_dir))\n",
    "    print (\"Working directory: {0}\".format(working_dir))\n",
    "    print (\"Pheno file: {0}\".format(args.model_file))\n",
    "    print (\"Number of iterations: {0}\".format(num_iterations))\n",
    "    print (\"Number of processors: {0}\".format(num_processors))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    file_list, num_contrasts, mat_file, con_file = model_setup(model_file, bids_dir, working_dir)\n",
    "\n",
    "    if args.analysis_level == \"participant\":\n",
    "        print(\"This bids-app does not support individual level analyses\")\n",
    "\n",
    "    elif args.analysis_level == \"group\":\n",
    "\n",
    "        import nipype.pipeline.engine as pe\n",
    "        import nipype.interfaces.fsl as fsl\n",
    "        import nipype.interfaces.io as nio\n",
    "        import nipype.interfaces.utility as niu\n",
    "        wf = pe.Workflow(name='wf_randomize')\n",
    "        wf.base_dir = working_dir\n",
    "\n",
    "        # First merge input files into single 4D file\n",
    "        merge = pe.Node(interface=fsl.Merge(), name='fsl_merge')\n",
    "        merge.inputs.in_files = file_list\n",
    "        merge.inputs.dimension = 't'\n",
    "        merge_output = \"randomise_pipe_merge.nii.gz\"\n",
    "        merge.inputs.merged_file = merge_output\n",
    "\n",
    "        # Create a mask from the merged file\n",
    "        mask = pe.Node(interface=fsl.maths.MathsCommand(), name='fsl_maths')\n",
    "        mask.inputs.args = '-abs -Tmin -bin'\n",
    "        merge_mask_output = \"randomise_pipe_mask.nii.gz\"\n",
    "        mask.inputs.out_file = merge_mask_output\n",
    "        wf.connect(merge, 'merged_file', mask, 'in_file')\n",
    "\n",
    "        # We want to parallelize so that each contrast is processed\n",
    "        # separately\n",
    "        def select(input_list):\n",
    "            out_file = input_list[0]\n",
    "            return out_file\n",
    "\n",
    "        for current_contrast in range(1, num_contrasts + 1):\n",
    "            # use randomize to use perform permutation test for contrast\n",
    "            randomise = pe.Node(interface=fsl.Randomise(), name='fsl_randomise_{0}'.format(current_contrast))\n",
    "            wf.connect(mask, 'out_file', randomise, 'mask')\n",
    "            randomise.inputs.base_name = \"randomise_pipe_contrast_{0}\".format(current_contrast)\n",
    "            randomise.inputs.design_mat = mat_file\n",
    "            randomise.inputs.tcon = con_file\n",
    "            randomise.inputs.args = ' --skipTo={0}'.format(current_contrast)\n",
    "            randomise.inputs.num_perm = num_iterations\n",
    "            randomise.inputs.demean = True\n",
    "            randomise.inputs.tfce = True\n",
    "            wf.connect(merge, 'merged_file', randomise, 'in_file')\n",
    "\n",
    "            select_t_corrected = pe.Node(niu.Function(input_names=[\"input_list\"],\n",
    "                                                      output_names=['out_file'],\n",
    "                                                      function=select),\n",
    "                                         name='select_t_cor{0}'.format(current_contrast))\n",
    "\n",
    "            wf.connect(randomise, \"t_corrected_p_files\", select_t_corrected, \"input_list\")\n",
    "\n",
    "            # threshold the resulting t corrected p file\n",
    "            thresh = pe.Node(interface=fsl.Threshold(),\n",
    "                             name='fsl_threshold_contrast_{0}'.format(current_contrast))\n",
    "            thresh.inputs.thresh = 0.95\n",
    "            wf.connect(select_t_corrected, \"out_file\", thresh, \"in_file\")\n",
    "            thresh_output_file = 'rando_pipe_thresh_tstat{0}.nii.gz'.format(current_contrast)\n",
    "            thresh.inputs.out_file = thresh_output_file\n",
    "\n",
    "            # binarize the result of applying the threshold to get a mask\n",
    "            thresh_bin = pe.Node(interface=fsl.maths.MathsCommand(),\n",
    "                                 name='fsl_threshold_bin_contrast_{0}'.format(current_contrast))\n",
    "            thresh_bin.inputs.args = '-bin'\n",
    "            wf.connect(thresh, \"out_file\", thresh_bin, \"in_file\")\n",
    "\n",
    "            select_t_stat = pe.Node(niu.Function(input_names=[\"input_list\"],\n",
    "                                                 output_names=['out_file'],\n",
    "                                                 function=select),\n",
    "                                    name='select_item_t_stat{0}'.format(current_contrast))\n",
    "\n",
    "            wf.connect(randomise, \"tstat_files\", select_t_stat, \"input_list\")\n",
    "\n",
    "            # apply calculated mask to the statistic image\n",
    "            apply_mask = pe.Node(interface=fsl.ApplyMask(),\n",
    "                                 name='fsl_applymask_contrast_{0}'.format(current_contrast))\n",
    "            wf.connect(select_t_stat, 'out_file', apply_mask, 'in_file')\n",
    "            wf.connect(thresh_bin, 'out_file', apply_mask, 'mask_file')\n",
    "\n",
    "            # cluster the results to get a report of the findings\n",
    "            cluster = pe.Node(interface=fsl.Cluster(),\n",
    "                              name='cluster_contrast_{0}'.format(current_contrast))\n",
    "            cluster.inputs.threshold = 0.0001\n",
    "            cluster.inputs.out_index_file = \"cluster_index_contrast_{0}\".format(current_contrast)\n",
    "            cluster.inputs.out_localmax_txt_file = \"lmax_contrast_{0}.txt\".format(current_contrast)\n",
    "            cluster.inputs.out_size_file = \"cluster_size_contrast_{0}\".format(current_contrast)\n",
    "            cluster.inputs.out_threshold_file = \"randomise_out_contrast_{0}\".format(current_contrast)\n",
    "            cluster.inputs.terminal_output = 'file'\n",
    "            wf.connect(apply_mask, 'out_file', cluster, 'in_file')\n",
    "\n",
    "            # attach a datasink to save the output\n",
    "            datasink = pe.Node(nio.DataSink(), name='sinker_contrast_{0}'.format(current_contrast))\n",
    "            datasink.inputs.base_directory = output_dir\n",
    "\n",
    "            wf.connect(apply_mask, 'out_file', datasink, 'output.@thresh_stat_file')\n",
    "            wf.connect(cluster, 'index_file', datasink, 'output.@index_file')\n",
    "            wf.connect(cluster, 'threshold_file', datasink, 'output.@threshold_file')\n",
    "            wf.connect(cluster, 'localmax_txt_file', datasink, 'output.@localmax_txt_file')\n",
    "            wf.connect(cluster, 'localmax_vol_file', datasink, 'output.@localmax_vol_file')\n",
    "            wf.connect(cluster, 'max_file', datasink, 'output.@max_file')\n",
    "            wf.connect(cluster, 'mean_file', datasink, 'output.@mean_file')\n",
    "            wf.connect(cluster, 'pval_file', datasink, 'output.@pval_file')\n",
    "            wf.connect(cluster, 'size_file', datasink, 'output.@size_file')\n",
    "\n",
    "        wf.run(plugin=\"MultiProc\", plugin_args={\"n_procs\": num_processors})\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
